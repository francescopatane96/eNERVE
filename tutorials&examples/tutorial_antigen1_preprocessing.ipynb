{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This colab sheet has the scope to guide the lecturer into preprocessing step of antigens and non antigens proteins. Then, data obtained from this step, will be use to develop a feedforward neural network model to predict new unseen antigens.\n",
        "\n",
        "- Positive and negative datasets were obtained from IEDB (https://www.iedb.org/) with following search parameters: 'epitope:any', 'organism:eucaryote(ID:2759)', 'host:human', 'assay:any', 'outcome:positive AND negative', 'MHC restriction:any', 'disease:infectious'.\n",
        "Remember to:\n",
        "- Download data in csv format\n",
        "- Clean data manually (human proteins are also downloaded, which can bias the data). "
      ],
      "metadata": {
        "id": "iBv12sKCkj8p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Bio"
      ],
      "metadata": {
        "id": "oB8WwxHAoTbU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import required libraries\n",
        "import csv\n",
        "import re"
      ],
      "metadata": {
        "id": "oIHe19szkYmL"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have a list of proteins but we must acquire their protein sequences to build a machine learning model. let's inspect csv file downloaded and iterate every instance to find uniprot link of the considered protein. "
      ],
      "metadata": {
        "id": "3Wo2dDaootQp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# open .csv file in reading mode\n",
        "with open('negative.csv', 'r') as csv_file:\n",
        "    # create a csv reader object to read the file \n",
        "    csv_reader = csv.reader(csv_file)\n",
        "\n",
        "    # create an empy list to contain valid rows \n",
        "    valid_rows = []\n",
        "\n",
        "    # iterate on every row of csv file\n",
        "    for row in csv_reader:\n",
        "        # verify if second column contains Verifichiamo se la seconda colonna contiene un link Uniprot valido\n",
        "        uniprot_link = re.findall(r'http://www.uniprot.org/uniprot/(\\w+)', row[1])\n",
        "        if uniprot_link:\n",
        "            # if a valid uniprot link is found, add the row to the list \n",
        "            row.append(uniprot_link[0])\n",
        "            valid_rows.append(row)\n",
        "    \n",
        "    # overwrite original csv file only with valid rows \n",
        "    with open('negative.csv', 'w', newline='') as new_csv_file:\n",
        "        csv_writer = csv.writer(new_csv_file)\n",
        "        csv_writer.writerows(valid_rows)\n"
      ],
      "metadata": {
        "id": "41hXwOA4wo8w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Do the same with positive dataset"
      ],
      "metadata": {
        "id": "EKPptaWxp-My"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# open .csv file in reading mode\n",
        "with open('positive.csv', 'r') as csv_file:\n",
        "    # create a csv reader object to read the file \n",
        "    csv_reader = csv.reader(csv_file)\n",
        "\n",
        "    # create an empy list to contain valid rows \n",
        "    valid_rows = []\n",
        "\n",
        "    # iterate on every row of csv file\n",
        "    for row in csv_reader:\n",
        "        # verify if second column contains Verifichiamo se la seconda colonna contiene un link Uniprot valido\n",
        "        uniprot_link = re.findall(r'http://www.uniprot.org/uniprot/(\\w+)', row[1])\n",
        "        if uniprot_link:\n",
        "            # if a valid uniprot link is found, add the row to the list \n",
        "            row.append(uniprot_link[0])\n",
        "            valid_rows.append(row)\n",
        "    \n",
        "    # overwrite original csv file only with valid rows \n",
        "    with open('positive.csv', 'w', newline='') as new_csv_file:\n",
        "        csv_writer = csv.writer(new_csv_file)\n",
        "        csv_writer.writerows(valid_rows)"
      ],
      "metadata": {
        "id": "Uw4-6qJYp7f1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For every protein we catch relate sequence and save it in a .fasta file."
      ],
      "metadata": {
        "id": "FyVR3NgHqNKs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1vWCiyKDY1Yk",
        "outputId": "29be8144-65ca-44ab-fdd7-39f4ee942757"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequenze aminoacidiche salvate in formato FASTA nel file \"proteine.fasta\"\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "\n",
        "# open csv file in reading mode\n",
        "with open('negative.csv', 'r') as csv_file:\n",
        "\n",
        "    # read csv \n",
        "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
        "\n",
        "    # skip the first line which contains header \n",
        "    next(csv_reader)\n",
        "\n",
        "    # open .fasta file in writing mode\n",
        "    with open('negative.fasta', 'w') as fasta_file:\n",
        "\n",
        "        # iterate between rows\n",
        "        for row in csv_reader:\n",
        "\n",
        "            # take URL of every protein from second column\n",
        "            url = row[1]\n",
        "\n",
        "            # extrapolate protein identifier from the URL\n",
        "            id_proteina = url.split('/')[-1]\n",
        "\n",
        "            # create protein URL\n",
        "            url_uniprot = 'https://www.uniprot.org/uniprot/{}.fasta'.format(id_proteina)\n",
        "\n",
        "            # download protein sequence from the URL\n",
        "            response = requests.get(url_uniprot)\n",
        "            seq = response.text.strip().split('\\n')[1:]\n",
        "\n",
        "            # write protein sequence in the .fasta file\n",
        "            fasta_file.write('>{}\\n{}\\n'.format(id_proteina, ''.join(seq)))\n",
        "\n",
        "print('aminoacidic sequences saved in the .FASTA file ''negative.fasta'') \n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since some proteins are shared between positive and negative datasets, it is best to remove them in order to facilitate the model in correctly predicting protein antigenicity."
      ],
      "metadata": {
        "id": "f8DWfU1XrJhy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import biopython library to handle protein sequences\n",
        "from Bio import SeqIO\n",
        "\n",
        "# define outputs names \n",
        "file1 = \"positive.fasta\"\n",
        "file2 = \"negative.fasta\"\n",
        "\n",
        "# create two empy sets to memorize protein sequences \n",
        "seqs1 = set()\n",
        "seqs2 = set()\n",
        "\n",
        "# open first fasta file and insert sequences into first set \n",
        "for record in SeqIO.parse(file1, \"fasta\"):\n",
        "    seqs1.add(str(record.seq))\n",
        "\n",
        "# do the same with second file \n",
        "for record in SeqIO.parse(file2, \"fasta\"):\n",
        "    seqs2.add(str(record.seq))\n",
        "\n",
        "# find shared proteins between two files and insert them into a new set \n",
        "shared_proteins = seqs1.intersection(seqs2)\n",
        "\n",
        "# remove shared proteins from original sets\n",
        "seqs1 = seqs1 - shared_proteins\n",
        "seqs2 = seqs2 - shared_proteins\n",
        "\n",
        "# create two new files to contain unique proteins for every class \n",
        "with open(\"unique_proteins_pos.fasta\", \"w\") as f:\n",
        "    for i, seq in enumerate(seqs1):\n",
        "        f.write(f\">protei_{i}\\n{seq}\\n\")\n",
        "        \n",
        "with open(\"unique_proteins_neg.fasta\", \"w\") as f:\n",
        "    for i, seq in enumerate(seqs2):\n",
        "        f.write(f\">protein_{i}\\n{seq}\\n\")\n",
        "\n",
        "print('number of antigens:', len(seqs1))\n",
        "print('number of NON antigens:', len(seqs2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m1kQ_QqB2NBK",
        "outputId": "50565d51-e98a-4fb5-e223-270484edb6f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "no of immunogens: 487\n",
            "no of non-immunogens: 369\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- BONUS ----> not very accurate"
      ],
      "metadata": {
        "id": "Ed3VRWQttv87"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data mining from NCBI"
      ],
      "metadata": {
        "id": "JfSycN0h7e3d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from Bio import Entrez, SeqIO\n",
        "\n",
        "# insert mail\n",
        "Entrez.email = \"francesco.patane@live.it\"\n",
        "\n",
        "# search on NCBI Protein\n",
        "handle = Entrez.esearch(db=\"protein\", term='(\"antigen\"[Function]) AND Aspergillus[Organism]', retmax=10000)   # select the organism\n",
        "record = Entrez.read(handle)\n",
        "\n",
        "# download seqs\n",
        "handle = Entrez.efetch(db=\"protein\", id=record[\"IdList\"], rettype=\"fasta\", retmode=\"text\")\n",
        "records = list(SeqIO.parse(handle, \"fasta\"))\n",
        "\n",
        "# save on file\n",
        "with open(\"antigens.fasta\", \"w\") as f:\n",
        "    SeqIO.write(records, f, \"fasta\")\n",
        "\n",
        "print(\"Downloaded seqs:\", len(records))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LmAcyoCXeJDB",
        "outputId": "a347b455-2fc3-499c-99fd-1ef1c83eb8f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Proteine scaricate: 9999\n"
          ]
        }
      ]
    }
  ]
}